{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30178f3f",
   "metadata": {},
   "source": [
    "\n",
    "# Trader Behaviour × Market Sentiment — Starter Notebook\n",
    "\n",
    "**Assignment**: Explore how trading behaviour (profitability, risk, volume, leverage) aligns/diverges from market sentiment (Fear vs Greed).  \n",
    "**Output**: Save processed CSVs to `csv_files/` and plots to `outputs/`. Export your report as **`ds_report.pdf`**.\n",
    "\n",
    "> Tip: Keep this notebook lean and reproducible. Avoid hard-coding file paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Always anchor to /content in Colab\n",
    "ROOT = Path(\"/content/ds_arkin_kansra\")\n",
    "CSV_DIR = ROOT / \"csv_files\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "\n",
    "# Make full directory tree\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Folders ready:\")\n",
    "print(\"ROOT:\", ROOT.resolve())\n",
    "print(\"CSV_DIR:\", CSV_DIR.resolve())\n",
    "print(\"OUT_DIR:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c9abf",
   "metadata": {},
   "source": [
    "## 1) Download Datasets from Google Drive (Public Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8913e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import io, os\n",
    "from pathlib import Path\n",
    "\n",
    "# === Anchor everything under /content/ds_arkin_kansra ===\n",
    "ROOT = Path(\"/content/ds_arkin_kansra\")\n",
    "CSV_DIR = ROOT / \"csv_files\"\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "drive_service = build('drive', 'v3')\n",
    "\n",
    "# Files to download: {file_id: local_path}\n",
    "files = {\n",
    "    \"1IAfLZwu6rJzyWKgBToqwSmmVYU6VbjVs\": str(CSV_DIR / \"hyperliquid_trades.csv\"),  # Trader data\n",
    "    \"1PgQC0tO8XN-wqkNyghWc_-mnrYv_nhSf\": str(CSV_DIR / \"fear_greed.csv\")           # Fear & Greed\n",
    "}\n",
    "\n",
    "for file_id, out_path in files.items():\n",
    "    try:\n",
    "        request = drive_service.files().get_media(fileId=file_id)\n",
    "        fh = io.FileIO(out_path, \"wb\")\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            if status:\n",
    "                print(f\"Downloading {out_path}: {int(status.progress() * 100)}%\")\n",
    "        print(f\" Finished downloading {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not download {out_path} from Drive. Please upload manually. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse trader timestamps\n",
    "if 'timestamp' in trades.columns:\n",
    "    ts_sample = trades['timestamp'].dropna().iloc[0]\n",
    "    if ts_sample > 1e11:  # ms\n",
    "        trades['time'] = pd.to_datetime(trades['timestamp'], unit='ms', errors='coerce', utc=True).dt.tz_convert(None)\n",
    "    else:  # s\n",
    "        trades['time'] = pd.to_datetime(trades['timestamp'], unit='s', errors='coerce', utc=True).dt.tz_convert(None)\n",
    "    trades['date'] = trades['time'].dt.date\n",
    "elif 'timestamp_ist' in trades.columns:\n",
    "    trades['time'] = pd.to_datetime(trades['timestamp_ist'], errors='coerce', dayfirst=True)\n",
    "    trades['date'] = trades['time'].dt.date\n",
    "else:\n",
    "    raise ValueError(\"No usable timestamp column found in trader dataset.\")\n",
    "\n",
    "print(\"Parsed trader dates:\", trades['date'].min(), \"→\", trades['date'].max(),\n",
    "      \"| unique days:\", trades['date'].nunique())\n",
    "\n",
    "# Sentiment dataset\n",
    "senti['date'] = pd.to_datetime(senti['date'], errors='coerce').dt.date\n",
    "print(\"Parsed sentiment dates:\", senti['date'].min(), \"→\", senti['date'].max(),\n",
    "      \"| n=\", len(senti))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36743375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column aliases\n",
    "col_map = {\n",
    "    \"size\": [\"size_usd\", \"size_tokens\", \"size\", \"qty\", \"quantity\", \"amount\"],\n",
    "    \"execution_price\": [\"execution_price\", \"price\", \"avg_price\"],\n",
    "    \"leverage\": [\"leverage\", \"lev\"],\n",
    "    \"closedpnl\": [\"closed_pnl\", \"pnl\", \"profit\", \"realized_pnl\"]\n",
    "}\n",
    "\n",
    "selected = {}\n",
    "for target, candidates in col_map.items():\n",
    "    for c in candidates:\n",
    "        if c in trades.columns:\n",
    "            selected[target] = c\n",
    "            break\n",
    "\n",
    "print(\"Detected columns:\", selected)\n",
    "\n",
    "# Normalize side\n",
    "if \"side\" in trades.columns:\n",
    "    trades[\"side\"] = trades[\"side\"].astype(str).str.lower().map({\"buy\":\"long\",\"sell\":\"short\"}).fillna(trades[\"side\"])\n",
    "\n",
    "# Profitability proxy\n",
    "if selected.get(\"closedpnl\"):\n",
    "    trades[\"is_win\"] = (trades[selected[\"closedpnl\"]] > 0).astype(int)\n",
    "else:\n",
    "    trades[\"is_win\"] = np.nan\n",
    "\n",
    "# Aggregations\n",
    "agg_funcs = {}\n",
    "if selected.get(\"size\"): agg_funcs[selected[\"size\"]] = \"sum\"\n",
    "if selected.get(\"execution_price\"): agg_funcs[selected[\"execution_price\"]] = \"mean\"\n",
    "if selected.get(\"leverage\"): agg_funcs[selected[\"leverage\"]] = \"mean\"\n",
    "agg_funcs[\"is_win\"] = \"mean\"\n",
    "if selected.get(\"closedpnl\"): agg_funcs[selected[\"closedpnl\"]] = [\"sum\",\"mean\",\"median\",\"std\"]\n",
    "\n",
    "daily = trades.groupby(\"date\").agg(agg_funcs)\n",
    "\n",
    "# Flatten + rename\n",
    "daily.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in daily.columns.values]\n",
    "rename_map = {}\n",
    "if selected.get(\"size\"): rename_map[f\"{selected['size']}_sum\"] = \"volume\"\n",
    "if selected.get(\"execution_price\"): rename_map[f\"{selected['execution_price']}_mean\"] = \"avg_price\"\n",
    "if selected.get(\"leverage\"): rename_map[f\"{selected['leverage']}_mean\"] = \"avg_leverage\"\n",
    "rename_map[\"is_win_mean\"] = \"win_rate\"\n",
    "if selected.get(\"closedpnl\"):\n",
    "    rename_map.update({\n",
    "        f\"{selected['closedpnl']}_sum\": \"pnl_sum\",\n",
    "        f\"{selected['closedpnl']}_mean\": \"pnl_mean\",\n",
    "        f\"{selected['closedpnl']}_median\": \"pnl_median\",\n",
    "        f\"{selected['closedpnl']}_std\": \"pnl_std\"\n",
    "    })\n",
    "daily = daily.rename(columns=rename_map).reset_index()\n",
    "\n",
    "# Long/Short imbalance\n",
    "if \"side\" in trades.columns and selected.get(\"size\"):\n",
    "    side_daily = trades.pivot_table(index=\"date\", columns=\"side\", values=selected[\"size\"], aggfunc=\"sum\", fill_value=0).reset_index()\n",
    "    if \"long\" not in side_daily.columns: side_daily[\"long\"] = 0.0\n",
    "    if \"short\" not in side_daily.columns: side_daily[\"short\"] = 0.0\n",
    "    side_daily[\"long_short_imbalance\"] = (side_daily[\"long\"] - side_daily[\"short\"]) / (side_daily[\"long\"] + side_daily[\"short\"]).replace(0, np.nan)\n",
    "    daily = daily.merge(side_daily[[\"date\",\"long\",\"short\",\"long_short_imbalance\"]], on=\"date\", how=\"left\")\n",
    "\n",
    "print(\"Daily features shape:\", daily.shape)\n",
    "print(daily.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti['classification'] = senti['classification'].str.strip().str.lower()\n",
    "senti['sentiment_flag'] = senti['classification'].map({\n",
    "    'fear': 0, 'extreme fear': 0,\n",
    "    'greed': 1, 'extreme greed': 1\n",
    "})\n",
    "\n",
    "df = pd.merge(\n",
    "    daily,\n",
    "    senti[['date','classification','value','sentiment_flag']],\n",
    "    on='date',\n",
    "    how='outer'   # keep all dates\n",
    ").sort_values('date')\n",
    "\n",
    "print(\"Final merged dataset shape:\", df.shape)\n",
    "print(\"Overlap rows (non-null both sides):\", df.dropna(subset=['volume','sentiment_flag']).shape)\n",
    "print(df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3754205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_plot(df, col, title, ylabel, fname):\n",
    "    if col in df.columns and df[col].notna().any():\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(pd.to_datetime(df['date']), df[col])\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Date\"); plt.ylabel(ylabel)\n",
    "        plt.savefig(OUT_DIR/fname, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Skipping {col} (not found or empty)\")\n",
    "\n",
    "# Trader-only metrics\n",
    "safe_plot(daily, \"volume\", \"Daily Trader Volume\", \"Volume\", \"daily_volume.png\")\n",
    "safe_plot(daily, \"avg_price\", \"Average Execution Price\", \"Avg Price\", \"avg_price.png\")\n",
    "safe_plot(daily, \"win_rate\", \"Win Rate\", \"Win Rate\", \"win_rate.png\")\n",
    "safe_plot(daily, \"pnl_sum\", \"PnL Sum\", \"PnL Sum\", \"pnl_sum.png\")\n",
    "\n",
    "# Sentiment trend\n",
    "safe_plot(senti, \"value\", \"Fear & Greed Index\", \"Index Value\", \"fear_greed_index.png\")\n",
    "\n",
    "# Overlap subset\n",
    "df_overlap = df.dropna(subset=['volume','sentiment_flag'])\n",
    "if not df_overlap.empty:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.scatter(df_overlap['sentiment_flag'], df_overlap['volume'])\n",
    "    plt.title(\"Overlap: Sentiment vs Trader Volume\")\n",
    "    plt.xlabel(\"Sentiment (0=Fear,1=Greed)\")\n",
    "    plt.ylabel(\"Volume\")\n",
    "    plt.savefig(OUT_DIR/\"overlap_sentiment_volume.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\" No overlapping days between sentiment & trader data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8788cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overlap = df.dropna(subset=['volume','sentiment_flag'])\n",
    "corrs = {}\n",
    "\n",
    "if not df_overlap.empty:\n",
    "    for col in ['volume','avg_price','win_rate','pnl_sum','pnl_mean','long_short_imbalance']:\n",
    "        if col in df_overlap.columns:\n",
    "            cor = df_overlap[col].corr(df_overlap['sentiment_flag'])\n",
    "            corrs[col] = cor\n",
    "\n",
    "    corr_df = pd.DataFrame.from_dict(corrs, orient='index', columns=['same_day_corr']).sort_values('same_day_corr', ascending=False)\n",
    "    corr_df.to_csv(CSV_DIR/'overlap_correlations.csv')\n",
    "    print(\"Overlap correlations:\")\n",
    "    print(corr_df.round(3))\n",
    "else:\n",
    "    print(\"No overlap rows to compute correlations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29927e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "if not df_overlap.empty:\n",
    "    for k in ['volume','avg_price','win_rate','pnl_sum','pnl_mean','long_short_imbalance']:\n",
    "        if k in df_overlap.columns:\n",
    "            x0 = df_overlap.loc[df_overlap['sentiment_flag']==0, k].dropna()\n",
    "            x1 = df_overlap.loc[df_overlap['sentiment_flag']==1, k].dropna()\n",
    "            if len(x0) > 1 and len(x1) > 1:  # lower threshold since data is tiny\n",
    "                t, p = stats.ttest_ind(x0, x1, equal_var=False)\n",
    "                results.append({'metric': k, 'fear_mean': x0.mean(), 'greed_mean': x1.mean(), 't_stat': t, 'p_value': p})\n",
    "\n",
    "if results:\n",
    "    regime_df = pd.DataFrame(results).sort_values('p_value')\n",
    "    regime_df.to_csv(CSV_DIR/'regime_differences.csv', index=False)\n",
    "    print(\"Significant regime differences (small-sample):\")\n",
    "    print(regime_df.round(3))\n",
    "else:\n",
    "    print(\" No regime differences computed (tiny overlap).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'n_trader_days': int(daily['date'].nunique()),\n",
    "    'trader_date_range': [str(daily['date'].min()), str(daily['date'].max())],\n",
    "    'n_sentiment_days': int(senti['date'].nunique()),\n",
    "    'sentiment_date_range': [str(senti['date'].min()), str(senti['date'].max())],\n",
    "    'n_overlap_days': int(df_overlap['date'].nunique()),\n",
    "}\n",
    "\n",
    "with open(CSV_DIR/'summary.json','w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Saved summary.json\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090260dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11: Rebuild outputs & generate ds_report.pdf ===\n",
    "import json, textwrap, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fpdf import FPDF\n",
    "from scipy import stats\n",
    "\n",
    "ROOT = Path(\"/content/ds_arkin_kansra\")\n",
    "CSV_DIR = ROOT / \"csv_files\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Rebuild summary.json if missing\n",
    "# -----------------------------\n",
    "if not (CSV_DIR/\"summary.json\").exists():\n",
    "    summary = {\n",
    "        'n_trader_days': int(daily['date'].nunique()),\n",
    "        'trader_date_range': [str(daily['date'].min()), str(daily['date'].max())],\n",
    "        'n_sentiment_days': int(senti['date'].nunique()),\n",
    "        'sentiment_date_range': [str(senti['date'].min()), str(senti['date'].max())],\n",
    "        'n_overlap_days': int(df.dropna(subset=['volume','sentiment_flag'])['date'].nunique()),\n",
    "    }\n",
    "    with open(CSV_DIR/'summary.json','w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(\"✅ Rebuilt summary.json\")\n",
    "\n",
    "with open(CSV_DIR/\"summary.json\") as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Rebuild correlations if missing\n",
    "# -----------------------------\n",
    "corr_path = CSV_DIR/\"overlap_correlations.csv\"\n",
    "if not corr_path.exists():\n",
    "    df_overlap = df.dropna(subset=['volume','sentiment_flag'])\n",
    "    corrs = {}\n",
    "    if not df_overlap.empty:\n",
    "        for col in ['volume','avg_price','win_rate','pnl_sum','pnl_mean','long_short_imbalance']:\n",
    "            if col in df_overlap.columns:\n",
    "                corrs[col] = df_overlap[col].corr(df_overlap['sentiment_flag'])\n",
    "        pd.DataFrame.from_dict(corrs, orient='index', columns=['same_day_corr']).to_csv(corr_path)\n",
    "        print(\"✅ Rebuilt overlap_correlations.csv\")\n",
    "\n",
    "corr_df = pd.read_csv(corr_path) if corr_path.exists() else None\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Rebuild regime differences if missing\n",
    "# -----------------------------\n",
    "regime_path = CSV_DIR/\"regime_differences.csv\"\n",
    "if not regime_path.exists():\n",
    "    results = []\n",
    "    if not df_overlap.empty:\n",
    "        for k in ['volume','avg_price','win_rate','pnl_sum','pnl_mean','long_short_imbalance']:\n",
    "            if k in df_overlap.columns:\n",
    "                x0 = df_overlap.loc[df_overlap['sentiment_flag']==0, k].dropna()\n",
    "                x1 = df_overlap.loc[df_overlap['sentiment_flag']==1, k].dropna()\n",
    "                if len(x0) > 1 and len(x1) > 1:\n",
    "                    t, p = stats.ttest_ind(x0, x1, equal_var=False)\n",
    "                    results.append({'metric': k, 'fear_mean': x0.mean(), 'greed_mean': x1.mean(), 't_stat': t, 'p_value': p})\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(regime_path, index=False)\n",
    "        print(\"✅ Rebuilt regime_differences.csv\")\n",
    "\n",
    "regime_df = pd.read_csv(regime_path) if regime_path.exists() else None\n",
    "\n",
    "# -----------------------------\n",
    "# 4. PDF Report\n",
    "# -----------------------------\n",
    "pdf = FPDF()\n",
    "pdf.set_left_margin(15)\n",
    "pdf.set_right_margin(15)\n",
    "pdf.add_page()\n",
    "\n",
    "# Title\n",
    "pdf.set_font(\"helvetica\", 'B', 16)\n",
    "pdf.cell(0, 10, \"Data Science Assignment Report\", new_x=\"LMARGIN\", new_y=\"NEXT\", align=\"C\")\n",
    "\n",
    "pdf.set_font(\"helvetica\", '', 12)\n",
    "pdf.ln(10)\n",
    "pdf.multi_cell(0, 8, \"Candidate: Arkin Kansra\")\n",
    "pdf.ln(5)\n",
    "\n",
    "# Section 1: Summary\n",
    "pdf.set_font(\"helvetica\", 'B', 14)\n",
    "pdf.cell(0, 10, \"1. Dataset Summary\", new_x=\"LMARGIN\", new_y=\"NEXT\")\n",
    "pdf.set_font(\"helvetica\", '', 12)\n",
    "\n",
    "for k, v in summary.items():\n",
    "    # convert list → comma-separated, everything to str\n",
    "    if isinstance(v, list):\n",
    "        val = \", \".join(map(str, v))\n",
    "    else:\n",
    "        val = str(v)\n",
    "\n",
    "    # clean up underscores and brackets\n",
    "    k_pretty = k.replace(\"_\", \" \").title()\n",
    "    safe_val = val.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\"_\",\" \")\n",
    "\n",
    "    line = f\"- {k_pretty}: {safe_val}\"\n",
    "\n",
    "    # hard wrap text safely\n",
    "    chunks = textwrap.wrap(line, width=90, break_long_words=True, break_on_hyphens=True)\n",
    "    if not chunks:  # edge case: empty line\n",
    "        chunks = [line]\n",
    "\n",
    "    for chunk in chunks:\n",
    "        pdf.multi_cell(190, 8, chunk)   # fixed width instead of 0\n",
    "pdf.ln(5)\n",
    "\n",
    "\n",
    "# Section 2: Correlations\n",
    "if corr_df is not None and not corr_df.empty:\n",
    "    pdf.set_font(\"helvetica\", 'B', 14)\n",
    "    pdf.cell(0, 10, \"2. Correlation Results\", new_x=\"LMARGIN\", new_y=\"NEXT\")\n",
    "    pdf.set_font(\"helvetica\", '', 12)\n",
    "\n",
    "    for _, row in corr_df.iterrows():\n",
    "        metric = row.iloc[0] if 'Unnamed: 0' in row.index else str(row.name)\n",
    "        safe_metric = metric.replace(\"_\", \" \")\n",
    "        line = f\"{safe_metric}: {row['same_day_corr']:.3f}\"\n",
    "\n",
    "        chunks = textwrap.wrap(line, width=90, break_long_words=True, break_on_hyphens=True)\n",
    "        if not chunks:\n",
    "            chunks = [line]\n",
    "        for chunk in chunks:\n",
    "            pdf.multi_cell(190, 8, chunk)\n",
    "    pdf.ln(5)\n",
    "\n",
    "\n",
    "# Section 3: Regime Differences\n",
    "if regime_df is not None and not regime_df.empty:\n",
    "    pdf.set_font(\"helvetica\", 'B', 14)\n",
    "    pdf.cell(0, 10, \"3. Regime Differences (Fear vs Greed)\", new_x=\"LMARGIN\", new_y=\"NEXT\")\n",
    "    pdf.set_font(\"helvetica\", '', 12)\n",
    "\n",
    "    for _, row in regime_df.iterrows():\n",
    "        safe_metric = str(row['metric']).replace(\"_\", \" \")\n",
    "        line = (f\"{safe_metric} | Fear mean={row['fear_mean']:.3f}, \"\n",
    "                f\"Greed mean={row['greed_mean']:.3f}, \"\n",
    "                f\"p={row['p_value']:.3f}\")\n",
    "\n",
    "        chunks = textwrap.wrap(line, width=90, break_long_words=True, break_on_hyphens=True)\n",
    "        if not chunks:\n",
    "            chunks = [line]\n",
    "        for chunk in chunks:\n",
    "            pdf.multi_cell(190, 8, chunk)\n",
    "    pdf.ln(5)\n",
    "\n",
    "# Section 4: Visualizations\n",
    "pdf.set_font(\"helvetica\", 'B', 14)\n",
    "pdf.cell(0, 10, \"4. Key Visualizations\", new_x=\"LMARGIN\", new_y=\"NEXT\")\n",
    "pdf.set_font(\"helvetica\", '', 12)\n",
    "pdf.multi_cell(0, 8, \"Below are the main charts generated during analysis:\")\n",
    "pdf.ln(5)\n",
    "\n",
    "for img_path in sorted(glob.glob(str(OUT_DIR/\"*.png\")) + glob.glob(str(OUT_DIR/\"*.jpg\"))):\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"helvetica\", 'I', 12)\n",
    "    pdf.cell(0, 10, f\"Figure: {Path(img_path).name}\", new_x=\"LMARGIN\", new_y=\"NEXT\")\n",
    "    pdf.image(img_path, x=15, w=180)\n",
    "\n",
    "# Save\n",
    "out_path = ROOT/\"ds_report.pdf\"\n",
    "pdf.output(str(out_path))\n",
    "print(f\"✅ Report generated at {out_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
